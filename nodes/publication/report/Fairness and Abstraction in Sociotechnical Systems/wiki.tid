audience:
authors.editors:[[andrew d. selbst]] [[danah boyd]] [[sorelle friedler]] [[suresh venkatasubramanian]] [[janet vertesi]]
created:1582720003601
date:
digital.harms.addressed:
element.type:publication
github.profile:
input.source:me2b
jurisdiction:
license:
modified:1582720003601
name:Fairness and Abstraction in Sociotechnical Systems
publication.type:report
purpose:
sector:
sponsoring.org:
tags:[[ai on the ground]] [[acm conference on fairness]] [[accountability]] [[and transparency]]
tech.focus:
tiddler.classification:node
title:Fairness and Abstraction in Sociotechnical Systems
tmap.edges:{}
tmap.id:480ce337-5a71-49e4-a4e8-20d7ae39b5bd
type:text/vnd.tiddlywiki
url:https://datasociety.net/output/fairness-and-abstraction-in-sociotechnical-systems/
version.or.edition:
volume.frequency:
working.group:

In this paper, authors identify the challenges to integrating fairness into machine learning based systems and suggest next steps.

    “In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five “traps” that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.”

